{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ad92a0",
   "metadata": {},
   "source": [
    "# Speech-to-Text Conversion Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6f1318",
   "metadata": {},
   "source": [
    "This script uses the OpenAI's Whisper model for speech-to-text tasks, handling command-line arguments for audio input and output paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54277c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import argparse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4756c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Argument parser setup\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--audio_path\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--output_path\", type=str, default=\"\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Setting device for model computation\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    # Model and processor setup\n",
    "    model_id = \"openai/whisper-large-v3\"\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "    # Pipeline setup for automatic speech recognition\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        max_new_tokens=256,\n",
    "        chunk_length_s=30,\n",
    "        batch_size=4,\n",
    "        return_timestamps=True,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    # Load and process the audio file\n",
    "    audio, sr = torchaudio.load(args.audio_path)\n",
    "    audio = audio.squeeze().numpy()\n",
    "\n",
    "    # Get results and write to file\n",
    "    result = pipe(audio)\n",
    "    with open(args.output_path, \"w\") as f:\n",
    "        f.write(result[\"text\"].strip())\n",
    "\n",
    "    # Print the transcribed text\n",
    "    print(result[\"text\"].strip())\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
