{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A02E0ePvwvLD",
        "outputId": "00522e81-06bc-41cf-c690-2e19f18e5f96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd /content/drive/'path'"
      ],
      "metadata": {
        "id": "1zUdUyK40ktW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml\n",
        "!pip install wordcloud\n",
        "!pip install imblearn\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJJ14hN3nQTb",
        "outputId": "7cfbd016-cb1e-4fd4-836a-3a0d053be2d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lxml\n",
            "  Downloading lxml-5.2.2-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lxml\n",
            "Successfully installed lxml-5.2.2\n",
            "Collecting wordcloud\n",
            "  Downloading wordcloud-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.1/511.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.25.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (10.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
            "Installing collected packages: wordcloud\n",
            "Successfully installed wordcloud-1.9.3\n",
            "Collecting imblearn\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Collecting imbalanced-learn (from imblearn)\n",
            "  Downloading imbalanced_learn-0.12.3-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
            "Installing collected packages: imbalanced-learn, imblearn\n",
            "Successfully installed imbalanced-learn-0.12.3 imblearn-0.0\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install faster-whisper\n",
        "# !pip install gradio\n",
        "# !pip install googletrans==4.0.0-rc1\n",
        "# !pip install --upgrade tensorflow"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ki0mfKdxfrmJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from faster_whisper import WhisperModel\n",
        "\n",
        "# stt_model = WhisperModel(\"large-v3\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "27XNYIsmwghy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # stt를 실행하는 함수\n",
        "# def stt_func(filepath):\n",
        "\n",
        "#   transcripts = []\n",
        "\n",
        "#   segments, info = stt_model.transcribe(filepath, language=\"ko\")\n",
        "#   tmp = \"\"\n",
        "#   for segment in segments:\n",
        "#     tmp = tmp + segment.text\n",
        "#     transcripts.append(tmp)\n",
        "\n",
        "#   return transcripts"
      ],
      "metadata": {
        "id": "oBRSFU_Bfv9B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# record = []\n",
        "# def trans_audio(filepath):\n",
        "#     segments, info = stt_model.transcribe(filepath, language=\"en\", task = \"transcribe\")\n",
        "#     tmp = \"\"\n",
        "#     for segment in segments:\n",
        "#         tmp = tmp + segment.text\n",
        "#         record.append(tmp)\n",
        "#     return tmp\n",
        "\n",
        "# import gradio as gr\n",
        "# import time\n",
        "\n",
        "# def recording():\n",
        "#   output_1 = gr.Textbox(label=\"Speech to Text\")\n",
        "\n",
        "#   gr.Interface(\n",
        "#     title = 'OpenAI Whisper ASR Gradio Web UI',\n",
        "#     fn = trans_audio,\n",
        "#     inputs=[gr.Audio(type=\"filepath\")],\n",
        "#     outputs = [output_1],\n",
        "#     live=True).launch()"
      ],
      "metadata": {
        "id": "2vg8Jfu-vQ-a"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdAsq3ah_oun",
        "outputId": "69183202-16fd-4cde-9c18-9e078478dad8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m636.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.2.2)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.6.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17395 sha256=6797cc9344f32b87e3d5f1f5f8f318711c0bfe2847407ac8239cb4451e44eada\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.6.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "# def translate_file(input_file_path, output_file_path, src_lang='ko', dest_lang='en'):\n",
        "#     # 번역기 초기화\n",
        "#     translator = Translator()\n",
        "\n",
        "#     # 입력 파일 읽기\n",
        "#     with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "#         korean_text = file.read()\n",
        "\n",
        "#     # 한국어 텍스트를 영어로 번역\n",
        "#     translation = translator.translate(korean_text, src=src_lang, dest=dest_lang)\n",
        "#     english_text = translation.text\n",
        "\n",
        "#     # 번역된 텍스트를 출력 파일로 저장\n",
        "#     with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "#         file.write(english_text)"
      ],
      "metadata": {
        "id": "406oGhVYwVhz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# CSV 파일 불러오기\n",
        "# data1 = pd.read_csv('/content/drive/MyDrive/MBTI 500.csv')\n",
        "# data2 = pd.read_csv('/content/drive/MyDrive/mbti_1.csv')\n",
        "\n",
        "# # MBTI_500과 mbti 데이터 합치기\n",
        "# data = pd.concat([data1, data2], ignore_index=True)\n",
        "\n",
        "# # 기존에 T 또는 F를 포함하는 성격 유형에 대해 해당 문자로 대체\n",
        "# data['type'] = data['type'].replace({\n",
        "#     'INTJ': 'T', 'INTP': 'T', 'ENTJ': 'T', 'ENTP': 'T',\n",
        "#     'ISTJ': 'T', 'ISTP': 'T', 'ESTJ': 'T', 'ESTP': 'T',\n",
        "#     'INFJ': 'F', 'INFP': 'F', 'ENFJ': 'F', 'ENFP': 'F',\n",
        "#     'ISFJ': 'F', 'ISFP': 'F', 'ESFJ': 'F', 'ESFP': 'F'\n",
        "# })\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Google Drive에 저장된 CSV 파일 경로\n",
        "file_path = '/content/drive/MyDrive/adasyn.csv'\n",
        "# CSV 파일 로드\n",
        "data = pd.read_csv(file_path, index_col=False)"
      ],
      "metadata": {
        "id": "XZAZ9-4jeD_7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 함수에서 사용할 contractions\n",
        "contractions = {\n",
        "    \"'cause\": 'because', \"I'd\": 'I would', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have',\n",
        "    \"I'm\": 'I am', \"I've\": 'I have', \"ain't\": 'is not', \"aren't\": 'are not', \"can't\": 'cannot', \"could've\": 'could have',\n",
        "    \"couldn't\": 'could not', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not',\n",
        "    \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \"he'll\": 'he will', \"he's\": 'he is', \"here's\": 'here is',\n",
        "    \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"i'd\": 'i would', \"i'd've\": 'i would have',\n",
        "    \"i'll\": 'i will', \"i'll've\": 'i will have', \"i'm\": 'i am', \"i've\": 'i have', \"isn't\": 'is not', \"it'd\": 'it would',\n",
        "    \"it'd've\": 'it would have', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it's\": 'it is', \"let's\": 'let us', \"ma'am\": 'madam',\n",
        "    \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have',\n",
        "    \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock',\n",
        "    \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"sha'n't\": 'shall not', \"shan't\": 'shall not', \"shan't've\": 'shall not have',\n",
        "    \"she'd\": 'she would', \"she'd've\": 'she would have', \"she'll\": 'she will', \"she'll've\": 'she will have', \"she's\": 'she is',\n",
        "    \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so's\": 'so as', \"so've\": 'so have',\n",
        "    \"that'd\": 'that would', \"that'd've\": 'that would have', \"that's\": 'that is', \"there'd\": 'there would', \"there'd've\": 'there would have',\n",
        "    \"there's\": 'there is', \"they'd\": 'they would', \"they'd've\": 'they would have', \"they'll\": 'they will', \"they'll've\": 'they will have',\n",
        "    \"they're\": 'they are', \"they've\": 'they have', \"this's\": 'this is', \"to've\": 'to have', \"wasn't\": 'was not', \"we'd\": 'we would',\n",
        "    \"we'd've\": 'we would have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not',\n",
        "    \"what'll\": 'what will', \"what'll've\": 'what will have', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have',\n",
        "    \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where's\": 'where is', \"where've\": 'where have', \"who'll\": 'who will',\n",
        "    \"who'll've\": 'who will have', \"who's\": 'who is', \"who've\": 'who have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have',\n",
        "    \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have',\n",
        "    \"y'all\": 'you all', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have',\n",
        "    \"you'd\": 'you would', \"you'd've\": 'you would have', \"you'll\": 'you will', \"you'll've\": 'you will have', \"you're\": 'you are', \"you've\": 'you have'\n",
        "}"
      ],
      "metadata": {
        "id": "iAcRpKXBhvtr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "def preprocess_sentence(sentence, remove_stopwords = True):\n",
        "    sentence = re.sub(r'https?:\\/\\/.*?[\\s+]', '', sentence)   # Links 제거\n",
        "    sentence = sentence.lower()                               # 텍스트 소문자화\n",
        "    sentence = BeautifulSoup(sentence, \"lxml\").text           # <br />, <a href = ...> 등의 html 태그 제거\n",
        "    sentence = re.sub(r'\\([^)]*\\)', '', sentence)             # 괄호로 닫힌 문자열  제거 ex) my friend(yugyeong) -> my friend\n",
        "    sentence = re.sub('\"','', sentence)                       # 쌍따옴표 \" 제거\n",
        "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
        "    sentence = re.sub(r\"'s\\b\",\"\",sentence)                    # 소유격 제거. ex) yugyeong's -> yugyeong\n",
        "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)             # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
        "    sentence = re.sub('[m]{2,}', 'mm', sentence)               # m이 3개 이상이면 2개로 변경. ex) ummmmmmm  -> umm\n",
        "\n",
        "    pers_types = ['infp' ,'infj', 'intp', 'intj', 'istp', 'isfp', 'isfj','istp',\n",
        "                  'entp', 'enfp', 'entj', 'enfj', 'estp', 'esfp' ,'esfj' ,'estj']\n",
        "    for types in pers_types:\n",
        "      sentence = sentence.replace(types, '')\n",
        "\n",
        "    # 불용어 제거 (Text)\n",
        "    if remove_stopwords:\n",
        "        tokens = ' '.join(word for word in sentence.split() if not word in stop_words if len(word) > 1)\n",
        "    # 불용어 미제거 (Summary)\n",
        "    else:\n",
        "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
        "    return tokens\n",
        "\n",
        "# # 데이터 프레임의 'posts' 컬럼에 전처리 적용\n",
        "# clean_posts = []\n",
        "# for s in data['posts']:\n",
        "#     clean_posts.append(preprocess_sentence(s))\n",
        "\n",
        "# # 전처리된 결과 확인\n",
        "# print(clean_posts[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCR8QUC1mS8Z",
        "outputId": "895dce91-496e-4bc2-ae2a-50e2f80904de"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data['posts'] = clean_posts\n",
        "# data"
      ],
      "metadata": {
        "id": "NIxFtiTgpTDd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type_counts = data['type'].value_counts()\n",
        "# type_counts"
      ],
      "metadata": {
        "id": "enylf0XmZbRL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data.isnull().sum()"
      ],
      "metadata": {
        "id": "nUoVsy0WpdgF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "# from wordcloud import WordCloud\n",
        "\n",
        "# # 타입 분포 히스토그램\n",
        "# plt.figure(figsize=(8, 2))\n",
        "# sns.countplot(data['type'])\n",
        "# plt.title('Distribution of MBTI Types (T/F)')\n",
        "# plt.xlabel('Type')\n",
        "# plt.ylabel('Count')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "DWbHCXmXrBYM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 포스트 길이 분포 히스토그램\n",
        "# post_lengths = data['posts'].apply(lambda x: len(x.split()))\n",
        "# plt.figure(figsize=(8, 4))\n",
        "# sns.histplot(post_lengths, bins=50, kde=True)\n",
        "# plt.title('Distribution of Post Lengths')\n",
        "# plt.xlabel('Number of Words in Post')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "gLu7hry3rH1y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 워드 클라우드\n",
        "# all_posts = ' '.join(data['posts'])\n",
        "# wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_posts)\n",
        "\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# plt.imshow(wordcloud, interpolation='bilinear')\n",
        "# plt.axis('off')\n",
        "# plt.title('Word Cloud of All Posts')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "hftZNqhTrJls"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 데이터 로드 및 분할\n",
        "X = data['posts']\n",
        "y = data['type']\n",
        "\n",
        "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TF-IDF 벡터화 및 모델 학습 파이프라인\n",
        "tfidf = TfidfVectorizer(lowercase=False)\n",
        "X_train_str = [str(x) for x in X_train]\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_str)\n",
        "\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "cv = GridSearchCV(clf, {'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}, scoring = \"accuracy\")\n",
        "\n",
        "train_clf = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(lowercase=False)),\n",
        "    ('clf', cv)\n",
        "])\n",
        "train_clf.fit(X_train_str, y_train)\n",
        "\n",
        "# 최적 파라미터로 재학습\n",
        "best_C = cv.best_estimator_.C\n",
        "print(f'best C: {best_C}')\n",
        "\n",
        "text_clf = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(lowercase=False)),\n",
        "    ('clf', LinearSVC(C=best_C))\n",
        "])\n",
        "text_clf.fit(X_train_str, y_train)\n",
        "\n",
        "# 검증 데이터로 성능 확인\n",
        "X_val_str = [str(x) for x in X_val]\n",
        "pred = text_clf.predict(X_val_str)\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val, pred):.4f}')\n",
        "\n",
        "# 테스트 데이터로 성능 확인\n",
        "X_test_str = [str(x) for x in X_test]\n",
        "pred_svc = text_clf.predict(X_test_str)\n",
        "print(f'Test Accuracy: {accuracy_score(y_test, pred_svc):.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXckPittkupm",
        "outputId": "17a02729-0bd2-42ad-82b7-e79288535244"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best C: 1.0\n",
            "Validation Accuracy: 0.9184\n",
            "Test Accuracy: 0.9207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# CalibratedClassifierCV로 LinearSVC 감싸기\n",
        "calibrated_svc = CalibratedClassifierCV(base_estimator=LinearSVC(C=best_C), method='sigmoid')\n",
        "\n",
        "# 전체 데이터로 모델 재학습\n",
        "X_str = [str(x) for x in X]\n",
        "svc_clf = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(lowercase=False)),\n",
        "    ('clf', calibrated_svc)\n",
        "])\n",
        "svc_clf.fit(X_str, y)\n",
        "pred_total_svc = svc_clf.predict(X_str)\n",
        "print(f'Total Model Accuracy: {accuracy_score(y, pred_total_svc):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtLKPNHIBatD",
        "outputId": "3c7d6b42-5cc0-4161-89fe-72f6ea7f1843"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Model Accuracy: 0.9714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 새로운 문장에 대한 예측 함수\n",
        "def predict_mbti_type(new_sentence):\n",
        "    prediction_proba = svc_clf.predict_proba([new_sentence])\n",
        "    prediction = svc_clf.predict([new_sentence])\n",
        "    return prediction_proba, prediction\n",
        "\n",
        "# 새로운 문장 예측 예시\n",
        "k = \"I love programming and solving complex problems.\"\n",
        "# k = preprocess_sentence(k)\n",
        "predicted_type = predict_mbti_type(k)\n",
        "print(f'The predicted MBTI type for the new sentence is: {predicted_type}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWcoM4PDr93U",
        "outputId": "e4d4b349-d3d1-44bb-fd9e-6c029098b3cd"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted MBTI type for the new sentence is: (array([[0.19958042, 0.80041958]]), array(['T'], dtype=object))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# 모델 저장\n",
        "joblib_file = \"calibrated_svc.pkl\"\n",
        "joblib.dump(svc_clf, joblib_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQOWu7-0HdhP",
        "outputId": "b8a59fcc-b9fe-487f-9702-c886096ee887"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['calibrated_svc.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.extmath import softmax\n",
        "\n",
        "record = []\n",
        "record.append('내가 오늘은 기분이 좋아')\n",
        "record.append('한국의 전통주은 쌀로 만들어집니다')\n",
        "record.append('내일 만나서 얘기하자')\n",
        "record.append('그랬구나, 많이 속상했구나')\n",
        "record.append('얼어붙은 한강 위로 고양이가 걸어다닙니다')\n",
        "record.append('배열에 상태공간을 추가해서 탐색할 수 있습니다')\n",
        "record.append('나 지금 너무 힘들다')\n",
        "record.append('기계학습이라는 과목은 평판이 나쁘다')\n",
        "record.append('맞아 나도 기계학습이라는 과목은 너무 싫어')\n",
        "record.append('중앙인증서버 공부나 하자')\n",
        "record.append('저 자동차는 검은색이네')\n",
        "record.append('마음이 너무 아파')\n",
        "\n",
        "\n",
        "for i in record:\n",
        "  print((i))\n",
        "  k = Translator().translate(i, src='ko', dest='en').text\n",
        "  print(k)\n",
        "\n",
        "  k = preprocess_sentence(k)\n",
        "  print(k)\n",
        "\n",
        "  predict = predict_mbti_type(k)\n",
        "  print(f\"predict_svm: {predict}\")\n",
        "\n",
        "\n",
        "  # predicted_type = 'T' if predict < 0.5 else 'F'\n",
        "  # print(f'Predicted MBTI type: {predicted_type}')\n",
        "  print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoHGD7My_wGQ",
        "outputId": "0d820cae-ed18-46d1-b2cf-ef18ca9aad69"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "내가 오늘은 기분이 좋아\n",
            "I feel good today\n",
            "feel good today\n",
            "predict_svm: (array([[0.99598445, 0.00401555]]), array(['F'], dtype=object))\n",
            "\n",
            "한국의 전통주은 쌀로 만들어집니다\n",
            "Korean traditional liquor is made of rice\n",
            "korean traditional liquor made rice\n",
            "predict_svm: (array([[0.11695276, 0.88304724]]), array(['T'], dtype=object))\n",
            "\n",
            "내일 만나서 얘기하자\n",
            "Let's meet tomorrow and talk\n",
            "let us meet tomorrow talk\n",
            "predict_svm: (array([[0.54658327, 0.45341673]]), array(['F'], dtype=object))\n",
            "\n",
            "그랬구나, 많이 속상했구나\n",
            "I did, you were a lot upset\n",
            "lot upset\n",
            "predict_svm: (array([[0.84513153, 0.15486847]]), array(['F'], dtype=object))\n",
            "\n",
            "얼어붙은 한강 위로 고양이가 걸어다닙니다\n",
            "The cat walks over the frozen Han River\n",
            "cat walks frozen han river\n",
            "predict_svm: (array([[0.03501887, 0.96498113]]), array(['T'], dtype=object))\n",
            "\n",
            "배열에 상태공간을 추가해서 탐색할 수 있습니다\n",
            "You can search by adding a state space to the array.\n",
            "search adding state space array\n",
            "predict_svm: (array([[0.33240393, 0.66759607]]), array(['T'], dtype=object))\n",
            "\n",
            "나 지금 너무 힘들다\n",
            "I'm so hard now\n",
            "hard\n",
            "predict_svm: (array([[0.97965876, 0.02034124]]), array(['F'], dtype=object))\n",
            "\n",
            "기계학습이라는 과목은 평판이 나쁘다\n",
            "The subject of machine learning is bad for reputation.\n",
            "subject machine learning bad reputation\n",
            "predict_svm: (array([[0.00935335, 0.99064665]]), array(['T'], dtype=object))\n",
            "\n",
            "맞아 나도 기계학습이라는 과목은 너무 싫어\n",
            "That's right, I hate the subject of machine learning so much\n",
            "right hate subject machine learning much\n",
            "predict_svm: (array([[0.0471144, 0.9528856]]), array(['T'], dtype=object))\n",
            "\n",
            "중앙인증서버 공부나 하자\n",
            "Let's study the central certification server\n",
            "let us study central certification server\n",
            "predict_svm: (array([[0.0276893, 0.9723107]]), array(['T'], dtype=object))\n",
            "\n",
            "저 자동차는 검은색이네\n",
            "That car is black\n",
            "car black\n",
            "predict_svm: (array([[0.15027831, 0.84972169]]), array(['T'], dtype=object))\n",
            "\n",
            "마음이 너무 아파\n",
            "My heart hurts so much\n",
            "heart hurts much\n",
            "predict_svm: (array([[0.91431138, 0.08568862]]), array(['F'], dtype=object))\n",
            "\n"
          ]
        }
      ]
    }
  ]
}